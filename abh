from clintraj_qi import *
from importlib import reload  
import math
import matplotlib
from matplotlib import gridspec
import matplotlib.colors as colors
import matplotlib.pyplot as plt
#%matplotlib notebook
import numpy as np
import pandas as pd
from pandas.plotting import scatter_matrix
from pathlib import Path
# seaborn can be used to "prettify" default matplotlib plots by importing and setting as default
import seaborn as sns
sns.set() # Set searborn as default
from scipy import linalg
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import scipy.stats
from sklearn import linear_model
from sklearn import preprocessing as preproc # load preprocessing function
from sklearn.cluster import KMeans
from sklearn.impute import KNNImputer
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
import sys
import time

df = pd.read_csv("MI.data", header=None, index_col=0)
display(df)

# Replace ? with NaN and convert to float
df = df.replace("?", np.nan)
df = df.apply(pd.to_numeric)

quantify_nans(df)

# Replace Nan with 0 and convert to float
df = df.replace(np.nan, 0)

sns.heatmap(df.corr())
plt.title("Correlation matrix")

df.describe()

# Plot distribution of all features
df.hist(figsize=(20,20))

##W9demo

# Make scatter matrix
scat_plt = scatter_matrix(df, figsize=(10,10))

def plotting(X, kmeans, y_pred, ax):
    ax.clear()
    ax.scatter(X[:,0], X[:,1], c = y_pred, marker= ".")
    ax.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = np.arange(kmeans.cluster_centers_.shape[0]), marker= "*", s=50)
    ax.set_title("Current iteration")

clusters = 8 # Control how many clusters we want

#fig, ax = plt.subplots(1,1)
#plt.suptitle("K-means Demo")

kmeans = KMeans(n_clusters = clusters, n_init = 1, random_state = 1, max_iter = 1)
y_pred = kmeans.fit_predict(df)

#ax.scatter(df[:,0], df[:,1], marker = '.', c = y_pred)

current_iter = 0
total_iter = 10
for _ in range(total_iter):
    kmeans= KMeans(n_clusters = clusters, init = "random", n_init = 1, random_state = 0, max_iter = 1 + current_iter)
    y_pred = kmeans.fit_predict(df)
    
#    plotting(df, kmeans, y_pred, ax)
#    fig.canvas.draw()
    
    current_iter += 1
    print(f'Iteration nr: {current_iter}.', end='\r')
    time.sleep(1)

##W9X3

X = df

[N, p] = X.shape

minX = list(np.min(X, axis=0)) # data range min
maxX = list(np.max(X, axis=0)) # data range max

clustersNr = 10
list_of_clusters = range(1,clustersNr + 1)
Nsim = 20
Wu = np.zeros((clustersNr, Nsim))
W = np.zeros(clustersNr)
for nrClusters in list_of_clusters: # Want actual number included
    kmeans = KMeans(n_clusters=nrClusters).fit(X)
    C = kmeans.cluster_centers_ # the cluster centers in the p dimensions
    labelCluster = kmeans.labels_ # the labelling for each point
    
    # Compute within-class dissimilarity given X (the data), C (the cluster centers)
    # and gr (the predicted cluster numbers)
    for cluster in range(1, nrClusters + 1):
        Ik = np.where(labelCluster == cluster - 1)[0]
        dk = np.sum((X[Ik, :] - np.multiply(np.ones((np.size(Ik), 1)), C[cluster - 1, :]))**2, axis = 1)  
        Dk = np.sum(dk)
        W[nrClusters-1] += Dk
    
    
    # gap-statistic
    # Nsim simulations of data uniformly distributed over [X]
    for j in range(Nsim):
        # simulate uniformly distributed data
        Xu = np.ones((N,1))*minX + np.random.rand(N,p)*(np.ones((N,1))*maxX-np.ones((N,1))*minX)
        # perform K-means
        kmeansU = KMeans(n_clusters=nrClusters).fit(Xu)
        Cu = kmeansU.cluster_centers_
        labelClusterU = kmeansU.labels_

        # Compute within-class dissmiliarity for the simulated data given Xu (the simulated data),
        # Cu (the cluster centers for the simulated data), and gru (the predicted cluster numbers)
        # for the simulated data).
        for cluster in range(1, nrClusters+1):
            Iku = np.where(labelClusterU == cluster - 1)
            dku = np.sum((Xu[Iku, :] - np.multiply(np.ones((np.size(Iku), 1)), Cu[cluster - 1, :]))**2, axis = 1)  
            Dku = np.sum(dku)
            Wu[nrClusters - 1, j] += Dku
        
# compute expectation of simulated within-class dissimilarities, and the 
# standard errors for the error bars
Elog_Wu = np.mean(np.log(Wu), axis = 1)
sk = np.std(np.log(Wu), axis=1)*np.sqrt(1+1/Nsim) # standard error sk' in (14.39)
x_range = np.array(range(nrClusters)) + 1

# Plot the log within class scatters
plt.figure()
plt.title("Within-class dissimilarity")
plt.plot(x_range, np.log(W), label='observed')
plt.plot(x_range, Elog_Wu, label='expected for simulation')
plt.legend(loc='upper left')
plt.xlabel("Number of clusters - k")
plt.ylabel("log(W)")
plt.show()

##W9E2

X=df
n, p = np.shape(X)

#For additional information concerning heirachical clustering in python, look at;
#https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/

d_sample = 'euclidean' #See possible values: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist
d_group = 'ward' #See possible values (method): https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html
N_leafs = 10 #Number of leaf nodes. n=400

Z = linkage(X, method=d_group, metric=d_sample) #method denotes cluster distance, metric denotes sample distance
plt.figure(figsize=(15,15))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('sample index')
plt.ylabel('distance')

#for scipy dendogram look at;
#https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html

den = dendrogram(
    Z,
    leaf_rotation=90.,
    leaf_font_size=8.,
    truncate_mode='lastp',
    p = N_leafs,
)
plt.show()



